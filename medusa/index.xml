<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Medusa on Medusa Docs</title>
    <link>https://ipsy-md.github.io/medusa-docs/medusa/</link>
    <description>Recent content in Medusa on Medusa Docs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://ipsy-md.github.io/medusa-docs/medusa/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Code of Conduct</title>
      <link>https://ipsy-md.github.io/medusa-docs/medusa/code_of_conduct/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipsy-md.github.io/medusa-docs/medusa/code_of_conduct/</guid>
      <description>Sharing is caring. —Wise Person Medusa is a shared resource, and therefore should be used with awareness and empathy for the work of others. Specific points to pay attention to are:
Use Condor for analysis All computational jobs are to be submitted to the HTCondor queue. The head node is meant for interactive use and quick computations, otherwise it negatively affects other people&#39;s work. Anything bigger should be submitted as an HTCondor Job, either as an interactive job or as a non-interactive job.</description>
    </item>
    
    <item>
      <title>Accessing</title>
      <link>https://ipsy-md.github.io/medusa-docs/medusa/access/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipsy-md.github.io/medusa-docs/medusa/access/</guid>
      <description>Command Line The easiest and most reliable way of connecting to Medusa is via SSH.
Connecting is as simple as running the following in your terminal:
ssh username&amp;#64;medusa.ovgu.de  NOTE: Users with unstable internet connections will likely find tmux to be a helpful tool.
 VPN You need a VPN connection to be able to log in to medusa. A description how to configure a VPN client can be found on the OVGU URZ page</description>
    </item>
    
    <item>
      <title>HTCondor</title>
      <link>https://ipsy-md.github.io/medusa-docs/medusa/htcondor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipsy-md.github.io/medusa-docs/medusa/htcondor/</guid>
      <description>This document focuses on how HTCondor is configured and intended to be used on our cluster. To learn about Condor and how to use it, you should read the Tools &amp;gt; HTCondor page.
HTCondor jobs come in two versions: interactive and non-interactive. Whenever possible, a non-interactive job should be used for computations.
TODO: This page needs some love.
The &amp;quot;Ideal&amp;quot; Job The &amp;quot;ideal&amp;quot; job is [1 CPU × 4 GiB] and runs for 1-3 hours.</description>
    </item>
    
    <item>
      <title>Data</title>
      <link>https://ipsy-md.github.io/medusa-docs/medusa/data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipsy-md.github.io/medusa-docs/medusa/data/</guid>
      <description>Folder Hierarchy All data in the /home directory is available across the entire cluster.
/home/&amp;lt;user_name&amp;gt; This directory is for all of your personal files. /home/data/&amp;lt;project_name&amp;gt; This directory is for data shared across the group/project. /home/&amp;lt;user_name&amp;gt;/scratch or /home/data/&amp;lt;project_name&amp;gt;/scratch This directory is not backed-up and should be used to store interim results which can be easily regenerated. Storing data here helps relieve the burden on backups. /home/data/archive/&amp;lt;project_name&amp;gt; Read-only and heavily compressed (via cool transparent compression mojo), this directory stores data for projects which are completed.</description>
    </item>
    
    <item>
      <title>Data Center</title>
      <link>https://ipsy-md.github.io/medusa-docs/medusa/data_center/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipsy-md.github.io/medusa-docs/medusa/data_center/</guid>
      <description>We have 5U in one of the racks in the G26 data center. It houses the backup server (Thunk).
  PWR U# Name Inventory # Overview   &amp;nbsp; 4 JBOD (Thunk) (likely part of 264097,000) 9x 4TB drives / 12x bays  5  &amp;nbsp; 6 Thunk 261309,000 2x 6-core 2.4 GHz Xeon E5645 96 GiB RAM
12x 4TB drives / 16x bays
  7  8    Also in G26, the URZ provided us with 2x 42U racks.</description>
    </item>
    
    <item>
      <title>Hardware</title>
      <link>https://ipsy-md.github.io/medusa-docs/medusa/hardware/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ipsy-md.github.io/medusa-docs/medusa/hardware/</guid>
      <description>Summary As of July 2020, the cluster comprises 15 nodes with over 250 CPU cores and 2.25 TiB of RAM. Centralized storage features more than 40 TiB of high performance SSD and 11 TiB of HDD capacity, and is accessed by cluster nodes via 10Gb Ethernet.
 Head Node (Medusa) 4x 8-core 2.8 GHz Opteron 6320 256 GiB RAM (16x 16GiB DDR3 ECC reg) 1x 10Gb NIC  Purchased 2013.12. Supermicro&#39;s specs: A+ Server 2042G-TRF.</description>
    </item>
    
  </channel>
</rss>